"""
This file contains the main code for the Streamlit app.
"""

import streamlit as st

# from src.crawler import WebCrawler
from src.services.indexer import Indexer
from src.utils.config import load_config


def init_session_state():
    """
    This function initializes the session state variables for the Streamlit app.
    It sets default values for URL, depth, and search results.
    """
    # -- Load configurations
    config = load_config("example_config.yaml")

    # -- Initialize session state variables
    if "indexer" not in st.session_state:
        st.session_state.indexer = Indexer(config["DATABASE"])

    if "config" not in st.session_state:
        st.session_state.config = config


def home_page():
    """
    Setup the home page of the Streamlit app with guidance.
    """
    # -- Initialize session state
    init_session_state()

    # -- Page title and description
    st.title("ğŸ” ScrapeSearch")
    st.markdown("### Busque, explore e analise conteÃºdo da web de forma automatizada.")

    st.write("""
    Bem-vindo ao **ScrapeSearch**, uma buscador com _web scraping_ para anÃ¡lise inteligente de documentos.
    
    Esta ferramenta permite:
    - ğŸ“¥ Coletar conteÃºdo de sites em diferentes nÃ­veis de profundidade.
    - ğŸ§  Analisar os textos extraÃ­dos com base em palavras-chave.
    - ğŸ“Š Classificar e armazenar os resultados automaticamente.
             
    O critÃ©rio baseado nas palavras-chave Ã© feito com base num cÃ¡lculo ponderado de:
    - Similaridade entre os textos e as palavras-chave.
    - FrequÃªncia de palavras-chave nos textos.
    - Posicionamento das palavras-chave nos textos (tÃ­tulo, parÃ¡grafo, etc.).
    - DistÃ¢ncia entre as palavras-chave.
    """)

    st.divider()

    st.markdown("## ğŸ› ï¸ Como usar:")

    with st.expander("Passo a passo para realizar uma busca", expanded=True):
        st.markdown("""
        1. Acesse a aba **Crawl Web** ğŸ•¸ï¸
            - Insira uma URL e escolha a profundidade de busca (0 a 2).
            - Clique em **Buscar** para iniciar o scraping.

        2. ApÃ³s a coleta, vÃ¡ para **Search** ğŸ”
            - Insira atÃ© duas palavras-chave para anÃ¡lise.
            - Clique em **Analisar Documentos**.
            - Veja os 10 resultados mais relevantes, ordenados por similaridade e contexto.

        3. Todas as informaÃ§Ãµes serÃ£o armazenadas no banco para consultas futuras.
        """)

    st.divider()
    st.caption("ğŸ§‘â€ğŸ’» Desenvolvido com Streamlit, spaCy e muito carinho.")


# Setting the pages
pages = [
    st.Page(home_page, title="Home", icon="ğŸ "),
    st.Page("src/pages/crawl_page.py", title="Crawl Web", icon="ğŸ•¸ï¸"),
    st.Page("src/pages/analyze_page.py", title="Search", icon="ğŸ”"),
]

# Running the app
pg = st.navigation({"ScrapeSearch": pages})
st.set_page_config(page_title="ScrapeSearch", page_icon="ğŸ”", layout="wide")
pg.run()
